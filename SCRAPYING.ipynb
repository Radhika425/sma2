AIzaSyBgMfvZcRewXnO_lP1jBv5vkJah2rdpKgg

pip install google-api-python-client

from googleapiclient.discovery import build
import pandas as pd

# Replace this with your own API key
API_KEY = 'AIzaSyBgMfvZcRewXnO_lP1jBv5vkJah2rdpKgg'
VIDEO_ID = 'dQw4w9WgXcQ'

def get_youtube_service():
    return build('youtube', 'v3', developerKey=API_KEY)

def get_video_comments(video_id, max_results=20):
    service = get_youtube_service()
    comments = []

    request = service.commentThreads().list(
        part="snippet",
        videoId=video_id,
        maxResults=max_results,
        textFormat="plainText"
    )
    response = request.execute()

    for item in response['items']:
        comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
        comments.append(comment)

    return comments
# Fetch first 20 comments
comments = get_video_comments(VIDEO_ID, max_results=20)

# Show the comments
for i, comment in enumerate(comments, 1):
    print(f"{i}. {comment}")

!pip install google-search-results nltk
from serpapi import GoogleSearch
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

# Replace with your actual SerpApi key
SERPAPI_KEY = "YOUR_SERPAPI_KEY"

# Sample Amazon product (can be changed later)
params = {
    "engine": "amazon_reviews",
    "api_key": SERPAPI_KEY,
    "amazon_domain": "amazon.in",
    "asin": "B0C7H57G1J",  # Replace with the ASIN of the product
    "sort_by": "recent",   # Options: recent, helpful
    "page": 1
}

# Fetch reviews
search = GoogleSearch(params)
results = search.get_dict()
reviews = results.get("reviews", [])

if not reviews:
    print("No reviews found or invalid ASIN/API key.")
else:
    # Initialize sentiment analyzer
    sid = SentimentIntensityAnalyzer()
    positive, negative, neutral = 0, 0, 0
    print("Customer Reviews:\n")
    for review in reviews[:20]:  # Limit to 20 reviews
        text = review.get("body", "")
        sentiment = sid.polarity_scores(text)
        print(f"Review: {text}")
        print(f"Sentiment Score: {sentiment}\n")

        # Count sentiment categories
        if sentiment['compound'] >= 0.05:
            positive += 1
        elif sentiment['compound'] <= -0.05:
            negative += 1
        else:
            neutral += 1
       # Summary
    print("Sentiment Summary:")
    print(f"Positive reviews: {positive}")
    print(f"Negative reviews: {negative}")
    print(f"Neutral reviews: {neutral}")
# 4d06902bd707e68d2f518ca37cc08ff2334ef6c54cbbc6141d4f05818bb2be77



import requests
from bs4 import BeautifulSoup
import pandas as pd
# Snapdeal product review URL (first page only)
url = "https://www.snapdeal.com/product/portronics-20000-mah-lipolymer-power/6917529658481881138/reviews?page=1"
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}
response = requests.get(url, headers=headers)
if response.status_code != 200:
    print("❌ Failed to fetch page.")
else:
    soup = BeautifulSoup(response.text, 'html.parser')
    review_blocks = soup.find_all('div', class_='user-review')
    if not review_blocks:
        print("❌ No reviews found.")
    else:
        reviews = []
        for review in review_blocks:
            reviewer_tag = review.find('span', class_='reviewer-name')
            title_tag = review.find('div', class_='head')
            content_tag = review.find('p', class_='review-text')
            rating_tag = review.find('div', class_='filled-stars')

            reviewer = reviewer_tag.get_text(strip=True) if reviewer_tag else "N/A"
            title = title_tag.get_text(strip=True) if title_tag else "N/A"
            content = content_tag.get_text(strip=True) if content_tag else "N/A"
            rating = rating_tag['style'].split(':')[1].strip().replace('%', '') if rating_tag else "N/A"

            reviews.append({
                "Reviewer": reviewer,
                "Title": title,
                "Review": content,
                "Rating (%)": rating
            })

        # Save to CSV
        df = pd.DataFrame(reviews)
        df.to_csv("snapdeal_reviews_page1.csv", index=False)
        print(f"✅ Scraped {len(df)} reviews. Saved to snapdeal_reviews_page1.csv")


!pip install selenium
!apt-get update # for ChromeDriver
!apt install chromium-chromedriver

import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup

# Set up Selenium with headless Chrome
chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")

driver = webdriver.Chrome(options=chrome_options)
# Load Snapdeal product URL
url = "https://www.snapdeal.com/product/shoetopia-beige-womens-loafers/6917529654811800170#bcrumbSearch:women%20shoe|bcrumbLabelId:227"
driver.get(url)
time.sleep(5)  # Wait for page & reviews to load
# Parse the fully loaded page with BeautifulSoup
soup = BeautifulSoup(driver.page_source, 'html.parser')

# Try to extract reviews
reviews = []
review_blocks = soup.find_all('div', class_='user-review')  # Check if any reviews are loaded
for block in review_blocks:
    reviewer = block.find('span', class_='reviewer-name')
    title = block.find('div', class_='head')
    content = block.find('p', class_='review-text')
    reviews.append({
        'Reviewer': reviewer.get_text(strip=True) if reviewer else 'Anonymous',
        'Title': title.get_text(strip=True) if title else 'No Title',
        'Content': content.get_text(strip=True) if content else 'No Content'
    })

driver.quit()

# Convert to DataFrame and save
df = pd.DataFrame(reviews)
df.to_csv("snapdeal_reviews1.csv", index=False)
print(f"✅ Total reviews scraped: {len(df)} — Saved to 'snapdeal_reviews1.csv'")
df.head()
